# Networking

## Installing cilium

Cilium is a *"framework for synamically managing networking"* (CNI - container network interface) which simplifies and controls communication between pods in a dynamic way. It manages layers 3, 4 (and 7) of the networking stack and includes a set of three sidecars per node, it uses eBPF to do this (eBPF is a kernel feature which allows seperation of environments), more on that here:[what is ebpf?](https://ebpf.io/what-is-ebpf/). They have an [install script](https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/#install-the-cilium-cli) which I've also pasted below:

```
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
```

At this point cilium cli is installed in `/usr/local/bin` you can update it as follows.
`/usr/local/bin/cilium install --version v1.18.0`

As cilium uses it's own proxy (envoy) you needn't run kube-proxy in parallel, the following step maybe needed:
`kubeadm init --skip-phases=addon/kube-proxy`

Once complete the pods should (eventually) start running:
```
kubectl get pods -A | grep cilium
kube-system   cilium-cl7g6                       1/1     Running   1 (66m ago)    3h37m
kube-system   cilium-envoy-bqzdk                 1/1     Running   1 (66m ago)    3h37m
kube-system   cilium-operator-6d4546b99d-nz86g   1/1     Running   1 (66m ago)    3h37m
```
Or use the cli:
```
root@labbox:~$ cilium status --wait
    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    OK
 \__/¯¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium                   Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet              cilium-envoy             Desired: 1, Ready: 1/1, Available: 1/1
Deployment             cilium-operator          Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium                   Running: 1
                       cilium-envoy             Running: 1
                       cilium-operator          Running: 1
                       clustermesh-apiserver    
                       hubble-relay             
Cluster Pods:          2/2 managed by Cilium
Helm chart version:    1.18.0
Image versions         cilium             quay.io/cilium/cilium:v1.18.0@sha256:dfea023972d06ec183cfa3c9e7809716f85daaff042e573ef366e9ec6a0c0ab2: 1
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.34.4-1753677767-266d5a01d1d55bd1d60148f991b98dac0390d363@sha256:231b5bd9682dfc648ae97f33dcdc5225c5a526194dda08124f5eded833bf02bf: 1
                       cilium-operator    quay.io/cilium/operator-generic:v1.18.0@sha256:398378b4507b6e9db22be2f4455d8f8e509b189470061b0f813f0fabaf944f51: 1
```

### Cilium sidecars

Sidecars run on a per node basis, their name role and purpose is defined below. 

| Name      | Role    | Purposes |
| --------- | ------- | --------|
| cilium | The Brain | eBPF setup for pod networking<br>Identity & label tracking<br>L3/L4 policy enforcement<br>Routing and NAT |
| operator | The networking control plane | Manages IP addressing<br>Garbage-collects identities<br>Coordinates cilium-agent behaviour|
| envoy | Layer 7 management and information | Layer 7 policies<br>HTTP/gRPC inspection<br>Hubble with L7 visibility |

## Istio

Istio is a *service mesh*. This takes care of layer 7 connectivity (things like mtls, weighted traffic, zero trust etc) and compliments cilium to some degree; however there are different methods of running the CNI and service mesh (some of the technologies overlap). Running istio by default will add a sidecar. As I'm short of resources and want a high tech POC, I'm going to do **CNI CHAINING** (I have a vague idea what this is...).

### Sidecar vs ambient

Istio can be setup with and without a sidecar (without is called ambiet mode). Given I'm short of resource I'll pick ambient mode. [Differences are detailed here](https://istio.io/latest/docs/overview/dataplane-modes/).

### Install

This can be completed as follows:
```
curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.26.3 sh -
mv istio-1.26.3/bin/istioctl /usr/local/bin/
rm -rf istio-1.26.3/
istioctl install --set profile=ambient -y
```

#### cni-exclusive problem

Then we have our first warning/error - this needs resolving. I vaguely remember seeing this mentioned in the [cilium documentation](https://docs.cilium.io/en/latest/network/servicemesh/istio/). 
```
❗ detected Cilium CNI with 'cni-exclusive=true'; this must be set to 'cni-exclusive=false' in the Cilium configuration
```

Fixing this is a bit cryptic if you've never used kubernetes, but it was quite simple - run this command:

`kubectl -n kube-system edit configmap cilium-config`

Look out for the line `cni-exclusive=false`  and set it to true then write quit to save. Job done, now we'll restart the daemonset.

`kubectl rollout restart daemonset cilium -n kube-system`

I'm going to restart the istio install and see if the error clears... 

#### Ztunnel pending/backoff state

Now I have a pod in a pending state.

It's worth noting that istio automatically creates a new namespace called `istio-system` be mindful of this going forward as not adding it to specific commands hides the output away.

```
root@labbox:~$ kubectl get pods -n istio-system
NAME                      READY   STATUS    RESTARTS   AGE
istio-cni-node-vs2d4      0/1     Running   0          5m24s
istiod-5889b8c87b-zmms6   0/1     Pending   0          10m
```
This pending node looks ominous - Let's run some diagnostics on it.

```
kubectl describe pod -n istio-system istiod-5889b8c87b-zmms6
<snip>
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  3m34s (x3 over 13m)  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
```

OK, googling this exact error [led me to this step](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#control-plane-node-isolation)
That looks simple enough and makes sense, some pods can't run on the control pane, you can sort that by taining the nodes. Good to know!

Taint by doing the following:
`kubectl taint nodes --all node-role.kubernetes.io/control-plane-`

OK, now I've restarted and ztunnel isn't running
```
root@labbox:~$ kubectl get pods -n istio-system -l app=ztunnel -o wide
NAME            READY   STATUS             RESTARTS     AGE   IP           NODE     NOMINATED NODE   READINESS GATES
ztunnel-lqbvz   0/1     CrashLoopBackOff   1 (4s ago)   5s    10.0.0.135   labbox   <none>           <none>
```

This looks more interesting; at this point there's no reason for it not to run, can we get logs out of the crashloop? Let's have a look. Note that if the crash loop is frequent, you can pass `--previous` to view output from the previous crashed container.

```kubectl logs -n istio-system ztunnel-lqbvz```

This outputs the logs and the breaks, I've trimmed the relevant bits so we can see whats happening:
```
2025-08-07T22:02:44.754344Z	warn	config	failed to determine if IPv6 was disabled; continuing anyways, but this may fail	err=Os { code: 2, kind: NotFound, message: "No such file or directory" }
2025-08-07T22:02:44.754576Z	info	ztunnel	version: version.BuildInfo{Version:"d4e543a299d5e1fb199449ed285758c090687452", GitRevision:"d4e543a299d5e1fb199449ed285758c090687452", RustVersion:"1.85.1", BuildProfile:"release", BuildStatus:"Clean", GitTag:"1.26.2-2-gd4e543a", IstioVersion:"1.26.3"}	
2025-08-07T22:02:44.754806Z	info	ztunnel	running with config: proxy: true
<snip>
statsAddr: !SocketAddr '[::]:15020'
readinessAddr: !SocketAddr '[::]:15021'
inboundAddr: '[::]:15008'
inboundPlaintextAddr: '[::]:15006'
outboundAddr: '[::]:15001'
<snip>
Caused by:
    Address family not supported by protocol (os error 97)
```

I've actually seen this error on the final line before `(os error 97)` ; it's due to an attempt at making a service listen on a IPv6 address - see the addresses in the middle? IPv6 addresses.

Given theres a warning on the first line *"No such file or directory"* I suspect this process is testing the contents of a `/proc` likely: `/proc/sys/net/ipv6/conf/all/disable_ipv6` that's fine... except we disabled IPv6 entirely, if it's disabled at a kernel level all the procs cease to exist. It's not just disabled - the kernels not even aware of it... This is a problem with the istio ztunnel code and I [raised it as an issue here](https://github.com/istio/ztunnel/issues/1611).

We can work around it by disabling IPv6 at an OS level and enabling at a kernel level. A painful resolution... 

... Incidently I asked chatGPT to write me some psuedocode to solve this in rust which it did [patch here](../code_samples/identity_v6)... *I didn't have the heart to add this as part of the issue though, it's not my code!*

So now we're going to enable IPv6 in grub and setup a sysctl to ensure it's disabled by the OS. This will keep all the /proc's in place and allow ztunnel to function.
```
sed -i 's/ipv6.disable=1//1' /etc/default/grub
update-grub
cat <<EOF >/etc/sysctl.d/100-disable-ipv6.conf 
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv6.conf.enp0s31f6.disable_ipv6 = 1
EOF
reboot
```

##### Fix confirmation

Now when the server comes back online we should have this file: `/proc/sys/net/ipv6/conf/all/disable_ipv6` which should reveal a 1 when catted. We should also (fingers crossed here) have ztunnel up and running. Output from my system below:
```
root@labbox:~$ cat /proc/sys/net/ipv6/conf/all/disable_ipv6 
1
root@labbox:~$ kubectl get pods -n istio-system -l app=ztunnel -o wide
NAME            READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
ztunnel-r64dl   1/1     Running   0          10h   10.0.0.50   labbox   <none>           <none>
```
